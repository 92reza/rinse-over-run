1.Who are you (mini-bio) and what do you do professionally?
    I am a software Engineer of about 9 years experience. I shifted to data science recently, about 6 months ago, and started working on projects at drivendata.org & topcoder.com websites. I am graduated of faculty of engineering in 2009, Communications and electronics department.

2.High level summary of your approach: what did you do and why?
    We predict turbidity of a process using multiple xgboost models, where each model predict turbidity based on data of a single phase or combinations of phases for that process. This approach is building optimized models based on testing data structure and evaluation metric. For more details, Check ReadMe.md in `src` directory

3.Copy and paste the 3 most impactful parts of your code and explain what each does and how it helped your model.

    a- in reshape.py
        phases_union_record.append(stats['mean'])
        phases_union_record.append(stats['std'])
        phases_union_record.append(stats['min'])
        phases_union_record.append(stats['25%'])
        phases_union_record.append(stats['50%'])
        phases_union_record.append(stats['75%'])
        phases_union_record.append(stats['max'])
        phases_union_record.append(col_values.head(5).mean())
        phases_union_record.append(col_values.tail(5).mean())
        phases_union_record.append(col_values.mad())

    This code get statistics for each field input. For more details about each statistics value, check below links
    https://www.geeksforgeeks.org/python-pandas-dataframe-describe-method/
    https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/

    b- in tune.py & train.py
        target_squared_inverse = 1/(target**2)
        target_squared_inverse_sum = np.sum(target_squared_inverse)
        target = target * (target_squared_inverse / target_squared_inverse_sum)
        target = np.sqrt(target)

    As xgboost optimize predictions based on mean sequare error(MSE), we need to use weighted values of target. Check below video for more details: 
    https://youtu.be/JaG-nFlU-jo?list=PLpQWTe-45nxL3bhyAJMEs90KF_gZmuqtm&t=310

    After using weighted values of target, I trained model against squart root of weighted value,  that produced better accuracy

    c- in predict.py
        def scale_value(value):
            if(value < threshold):
                return value*.962
            else:
                return value*.931

    When I scaled down the final predicted value, I got improvement in score, because MAPE gives more weight for lower values, even after using the variant version.


4.What are some other things you tried that didn’t necessarily make it into the final workflow (quick overview)?
    I tried to predict turbidity for each timestamp of a process , then get median for these prediction, but final error was about .47
    Also I tried getting statistics for full process timestamps, like benchmark solution, but I got error of about .53 


5.Did you use any tools for data preparation or exploratory data analysis that aren’t listed in your code submission?
    No

6.How did you evaluate performance of the model other than the provided metric, if at all?
    - Usually I evaluate model based on accuracy and speed
    - With respect to speed, the time of execution for full workflow model(reshaping, tunning, training, prediction) was about 90 minutes on my machine(Check ReadMe for specs), which I think it's good.


7.Anything we should watch out for or be aware of in using your model (e.g. code quirks, memory requirements, numerical stability issues, etc.)?
    When you execute `run.bat` in `src` directory to run all steps in single command, make sure to change paths to valid paths where raw data exists in your machine. Also, If you are running linux OS, change paths to be unix style.


8.Do you have any useful charts, graphs, or visualizations from the process?
    No

9.If you were to continue working on this problem for the next year, what methods or techniques might you try in order to build on your work so far? Are there other fields or features you felt would have been very helpful to have?
    - Use Deep learning
    - Check Model using RandomForest
    - Use LightGBM for faster training